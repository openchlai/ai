services:
  # Redis for Celery backend
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Main FastAPI application
  ai-pipeline:
    build: .
    ports:
      - "8123:8123"
      - "8300:8300"
    depends_on:
      redis:
        condition: service_healthy
    environment:
      - DEBUG=true
      - LOG_LEVEL=INFO
      - SITE_ID=dev-local
      - ENABLE_MODEL_LOADING=false  # API server doesn't need models
      - REDIS_URL=redis://redis:6379/0
      - REDIS_TASK_DB=1
      - DOCKER_CONTAINER=true
      - ENABLE_STREAMING=${ENABLE_STREAMING:-false}
      - MAX_STREAMING_SLOTS=${MAX_STREAMING_SLOTS:-2}
      - MAX_BATCH_SLOTS=${MAX_BATCH_SLOTS:-1}
      - STREAMING_PORT=${STREAMING_PORT:-8300}
      - STREAMING_HOST=${STREAMING_HOST:-0.0.0.0}
      - REDIS_STREAMING_DB=${REDIS_STREAMING_DB:-2}  # Separate Redis DB for streaming
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
      - ./temp:/app/temp
      - ./logs/streaming:/app/logs/streaming
    command: >
      sh -c "
        echo 'Container Environment Check:';
        echo 'ENABLE_STREAMING='\"$$ENABLE_STREAMING\";
        echo 'Command args: enable-streaming='\"$$ENABLE_STREAMING\";
        if [ \"$$ENABLE_STREAMING\" = \"true\" ]; then
          echo 'üéôÔ∏è Starting AI Pipeline with Streaming Support...';
          python -m app.main --enable-streaming;
        else
          echo 'üì¶ Starting AI Pipeline (Batch Only mode)...';
          python -m app.main;
        fi
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery worker for processing tasks
  celery-worker:
    build: .
    depends_on:
      redis:
        condition: service_healthy
    environment:
      - DEBUG=true
      - LOG_LEVEL=INFO
      - ENABLE_MODEL_LOADING=true  # Worker needs models
      - NUMBA_CACHE_DIR=/tmp/numba_cache  # Writable cache location
      - NUMBA_DISABLE_JIT=0               # Keep JIT enabled
      - NUMBA_DISABLE_CACHING=1           # Disable persistent caching 
      - REDIS_URL=redis://redis:6379/0
      - REDIS_TASK_DB=1
      - DOCKER_CONTAINER=true
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
      - ./temp:/app/temp
    command: celery -A app.celery_app worker --loglevel=info -E --pool=solo --hostname=worker@%h
    # Production command with concurrency and GPU support
    # command: celery -A app.celery_app worker --loglevel=info -E --pool=prefork --concurrency=2 --hostname=worker@%h
    restart: unless-stopped
    deploy:
      replicas: 1  # This sets the default number of worker replicas to 3
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G
    healthcheck:
      test: ["CMD", "celery", "-A", "app.celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: Celery monitoring (Flower)
  flower:
    build: .
    depends_on:
      - redis
      - celery-worker
    environment:
      - REDIS_URL=redis://redis:6379/0
    ports:
      - "5555:5555"
    command: celery -A app.celery_app flower --port=5555
    restart: unless-stopped

volumes:
  redis_data: