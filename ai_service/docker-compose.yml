services:
  # Redis for Celery backend
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Main FastAPI application
  ai-pipeline:
    build: .
    ports:
      - "${APP_PORT:-8125}:${APP_PORT:-8125}"
      - "${STREAMING_PORT:-8300}:${STREAMING_PORT:-8300}"
    depends_on:
      redis:
        condition: service_healthy
    environment:
      - APP_PORT=${APP_PORT:-8125}
      - DEBUG=${DEBUG:-true}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - SITE_ID=${SITE_ID:-dev-local}
      - ENABLE_MODEL_LOADING=false  # API server doesn't need models
      - REDIS_URL=redis://redis:6379/0
      - REDIS_TASK_DB=${REDIS_TASK_DB:-1}
      - DOCKER_CONTAINER=true
      - ENABLE_STREAMING=${ENABLE_STREAMING:-false}
      - MAX_STREAMING_SLOTS=${MAX_STREAMING_SLOTS:-2}
      - MAX_BATCH_SLOTS=${MAX_BATCH_SLOTS:-1}
      - STREAMING_PORT=${STREAMING_PORT:-8300}
      - STREAMING_HOST=${STREAMING_HOST:-0.0.0.0}
      - REDIS_STREAMING_DB=${REDIS_STREAMING_DB:-2}
      # HuggingFace Configuration
      - HF_TOKEN=${HF_TOKEN}
      - USE_HF_MODELS=${USE_HF_MODELS:-true}
      - HF_ORGANIZATION=${HF_ORGANIZATION:-openchs}
      - HF_ASR_MODEL=${HF_ASR_MODEL}
      - HF_CLASSIFIER_MODEL=${HF_CLASSIFIER_MODEL}
      - HF_NER_MODEL=${HF_NER_MODEL}
      - HF_TRANSLATOR_MODEL=${HF_TRANSLATOR_MODEL}
      - HF_SUMMARIZER_MODEL=${HF_SUMMARIZER_MODEL}
      - HF_QA_MODEL=${HF_QA_MODEL}
      - HF_HOME=/app/.cache/huggingface
    volumes:
      - ./logs:/app/logs
      - ./temp:/app/temp
      - ./logs/streaming:/app/logs/streaming
    command: >
      sh -c "
        echo 'Container Environment Check:';
        echo 'ENABLE_STREAMING='\"$$ENABLE_STREAMING\";
        echo 'Command args: enable-streaming='\"$$ENABLE_STREAMING\";
        if [ \"$$ENABLE_STREAMING\" = \"true\" ]; then
          echo 'üéôÔ∏è Starting AI Pipeline with Streaming Support...';
          python -m app.main --enable-streaming;
        else
          echo 'üì¶ Starting AI Pipeline (Batch Only mode)...';
          python -m app.main;
        fi
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:$${APP_PORT:-8125}/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery worker for processing tasks
  celery-worker:
    build: .
    depends_on:
      redis:
        condition: service_healthy
    environment:
      - DEBUG=${DEBUG:-true}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ENABLE_MODEL_LOADING=true
      - NUMBA_CACHE_DIR=/tmp/numba_cache
      - NUMBA_DISABLE_JIT=0
      - NUMBA_DISABLE_CACHING=1
      - REDIS_URL=redis://redis:6379/0
      - REDIS_TASK_DB=${REDIS_TASK_DB:-1}
      - DOCKER_CONTAINER=true
      # HuggingFace Configuration
      - HF_TOKEN=${HF_TOKEN}
      - USE_HF_MODELS=${USE_HF_MODELS:-true}
      - HF_ORGANIZATION=${HF_ORGANIZATION:-openchs}
      - HF_ASR_MODEL=${HF_ASR_MODEL}
      - HF_CLASSIFIER_MODEL=${HF_CLASSIFIER_MODEL}
      - HF_NER_MODEL=${HF_NER_MODEL}
      - HF_TRANSLATOR_MODEL=${HF_TRANSLATOR_MODEL}
      - HF_SUMMARIZER_MODEL=${HF_SUMMARIZER_MODEL}
      - HF_QA_MODEL=${HF_QA_MODEL}
      - HF_HOME=/app/.cache/huggingface
    volumes:
      - ./logs:/app/logs
      - ./temp:/app/temp
      - huggingface_cache:/app/.cache/huggingface  # Docker-managed volume for HuggingFace cache
    # IMPORTANT: Must specify -Q model_processing,celery to consume from correct queues
    command: celery -A app.celery_app worker --loglevel=info -E --pool=solo -Q model_processing,celery --hostname=worker@%h
    # Production command with concurrency and GPU support
    # command: celery -A app.celery_app worker --loglevel=info -E --pool=prefork --concurrency=2 -Q model_processing,celery --hostname=worker@%h
    restart: unless-stopped
    deploy:
      replicas: 1  # This sets the default number of worker replicas to 3
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G
    healthcheck:
      test: ["CMD", "celery", "-A", "app.celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: Celery monitoring (Flower)
  flower:
    build: .
    depends_on:
      - redis
      - celery-worker
    environment:
      - REDIS_URL=redis://redis:6379/0
      - REDIS_TASK_DB=${REDIS_TASK_DB:-1}
      - DOCKER_CONTAINER=true
      # HuggingFace Configuration (needed for task module imports)
      - HF_TOKEN=${HF_TOKEN}
      - USE_HF_MODELS=${USE_HF_MODELS:-true}
      - HF_ORGANIZATION=${HF_ORGANIZATION:-openchs}
      - HF_ASR_MODEL=${HF_ASR_MODEL}
      - HF_CLASSIFIER_MODEL=${HF_CLASSIFIER_MODEL}
      - HF_NER_MODEL=${HF_NER_MODEL}
      - HF_TRANSLATOR_MODEL=${HF_TRANSLATOR_MODEL}
      - HF_SUMMARIZER_MODEL=${HF_SUMMARIZER_MODEL}
      - HF_QA_MODEL=${HF_QA_MODEL}
      - HF_HOME=/app/.cache/huggingface
    ports:
      - "5555:5555"
    command: celery -A app.celery_app flower --port=5555
    restart: unless-stopped

volumes:
  redis_data:
  huggingface_cache: