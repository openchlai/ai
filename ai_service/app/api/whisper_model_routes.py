# """
# API endpoints for Whisper model management and dynamic switching
# """
# from fastapi import APIRouter, HTTPException
# from typing import Dict, Any
# from pydantic import BaseModel
# import logging
# from datetime import datetime

# from ..core.whisper_model_manager import whisper_model_manager, WhisperVariant, TranslationStrategy

# logger = logging.getLogger(__name__)

# router = APIRouter(prefix="/api/v1/models/whisper", tags=["Whisper Model Management"])

# # Pydantic models for requests
# class WhisperSwitchRequest(BaseModel):
#     """Request model for switching Whisper configuration"""
#     variant: str  # "large_v3" or "large_turbo"
#     strategy: str  # "whisper_builtin" or "custom_model"

# class WhisperTranscribeRequest(BaseModel):
#     """Request model for transcription with strategy"""
#     text: str
#     language: str = "sw"

# @router.get("/status")
# async def get_whisper_status():
#     """Get current Whisper model status and configuration"""
#     try:
#         status = await whisper_model_manager.get_model_status()
#         return {
#             "timestamp": datetime.now().isoformat(),
#             "whisper_status": status
#         }
#     except Exception as e:
#         logger.error(f"‚ùå Failed to get Whisper status: {e}")
#         raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")

# @router.get("/configurations")
# async def get_available_configurations():
#     """Get all available Whisper variant and translation strategy combinations"""
#     try:
#         return {
#             "available_combinations": whisper_model_manager.get_available_combinations(),
#             "current_configuration": {
#                 "variant": whisper_model_manager.current_variant.value if whisper_model_manager.current_variant else None,
#                 "strategy": whisper_model_manager.current_strategy.value if whisper_model_manager.current_strategy else None
#             },
#             "model_paths": {
#                 "large_v3": whisper_model_manager.variant_paths[WhisperVariant.LARGE_V3],
#                 "large_turbo": whisper_model_manager.variant_paths[WhisperVariant.LARGE_TURBO]
#             }
#         }
#     except Exception as e:
#         logger.error(f"‚ùå Failed to get configurations: {e}")
#         raise HTTPException(status_code=500, detail=f"Failed to get configurations: {str(e)}")

# @router.post("/switch")
# async def switch_whisper_configuration(request: WhisperSwitchRequest):
#     """Switch Whisper model variant and translation strategy"""
#     try:
#         logger.info(f"üîÑ Switching Whisper to {request.variant} + {request.strategy}")
        
#         # Perform the switch
#         success, message = await whisper_model_manager.switch_configuration(request.variant, request.strategy)
        
#         if success:
#             return {
#                 "success": True,
#                 "message": message,
#                 "new_configuration": {
#                     "variant": whisper_model_manager.current_variant.value,
#                     "strategy": whisper_model_manager.current_strategy.value
#                 },
#                 "timestamp": datetime.now().isoformat()
#             }
#         else:
#             raise HTTPException(status_code=400, detail=message)
            
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"‚ùå Failed to switch Whisper configuration: {e}")
#         raise HTTPException(status_code=500, detail=f"Failed to switch configuration: {str(e)}")

# @router.post("/reload")
# async def reload_whisper_model():
#     """Force reload the current Whisper model (useful after manual model updates)"""
#     try:
#         logger.info("üîÑ Force reloading current Whisper model")
        
#         success = await whisper_model_manager.load_whisper_model(force_reload=True)
        
#         if success:
#             return {
#                 "success": True,
#                 "message": f"Successfully reloaded Whisper {whisper_model_manager.current_variant.value}",
#                 "configuration": whisper_model_manager.get_loaded_variant_info(),
#                 "timestamp": datetime.now().isoformat()
#             }
#         else:
#             raise HTTPException(status_code=500, detail="Failed to reload Whisper model")
            
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"‚ùå Failed to reload Whisper model: {e}")
#         raise HTTPException(status_code=500, detail=f"Failed to reload model: {str(e)}")

# # @router.get("/translation-strategy")
# # async def get_translation_strategy():
# #     """Get current translation strategy and recommendations"""
# #     try:
# #         current_variant = whisper_model_manager.current_variant
# #         current_strategy = whisper_model_manager.current_strategy
        
# #         return {
# #             "current_strategy": current_strategy.value if current_strategy else None,
# #             "current_variant": current_variant.value if current_variant else None,
# #             "uses_whisper_translation": whisper_model_manager.should_use_whisper_translation(),
# #             "uses_custom_translation": whisper_model_manager.should_use_custom_translation(),
# #             "recommended_strategy": whisper_model_manager.get_recommended_translation_strategy(current_variant).value if current_variant else None,
# #             "strategy_explanation": {
# #                 "whisper_builtin": "Use Whisper large-v3's built-in translation (faster, one model)",
# #                 "custom_model": "Use separate custom translation model (more control, supports any whisper variant)"
# #             }
# #         }
        
# #     except Exception as e:
# #         logger.error(f"‚ùå Failed to get translation strategy: {e}")
# #         raise HTTPException(status_code=500, detail=f"Failed to get strategy: {str(e)}")

# @router.post("/test-transcription")
# async def test_current_configuration(language: str = "sw"):
#     """Test transcription with current Whisper configuration"""
#     try:
#         if not whisper_model_manager.is_model_loaded():
#             raise HTTPException(status_code=503, detail="No Whisper model loaded")
        
#         # Create test audio data (small silent audio)
#         import numpy as np
#         import io
#         import wave
        
#         # Generate 1 second of silence at 16kHz
#         sample_rate = 16000
#         duration = 1.0
#         samples = np.zeros(int(sample_rate * duration), dtype=np.float32)
        
#         # Convert to WAV bytes
#         buffer = io.BytesIO()
#         with wave.open(buffer, 'wb') as wav_file:
#             wav_file.setnchannels(1)  # Mono
#             wav_file.setsampwidth(2)  # 16-bit
#             wav_file.setframerate(sample_rate)
#             wav_file.writeframes((samples * 32767).astype(np.int16).tobytes())
        
#         test_audio_bytes = buffer.getvalue()
        
#         # Test transcription
#         result = whisper_model_manager.transcribe_with_strategy(test_audio_bytes, language)
        
#         return {
#             "test_successful": True,
#             "configuration": {
#                 "variant": whisper_model_manager.current_variant.value,
#                 "strategy": whisper_model_manager.current_strategy.value
#             },
#             "test_result": result,
#             "timestamp": datetime.now().isoformat()
#         }
        
#     except Exception as e:
#         logger.error(f"‚ùå Transcription test failed: {e}")
#         raise HTTPException(status_code=500, detail=f"Transcription test failed: {str(e)}")

# @router.get("/memory-usage")
# async def get_whisper_memory_usage():
#     """Get memory usage information for Whisper models"""
#     try:
#         import torch
#         import psutil
#         import os
        
#         memory_info = {
#             "system_memory": {
#                 "total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
#                 "available_gb": round(psutil.virtual_memory().available / (1024**3), 2),
#                 "used_percent": psutil.virtual_memory().percent
#             },
#             "process_memory": {
#                 "current_process_mb": round(psutil.Process(os.getpid()).memory_info().rss / (1024**2), 2)
#             }
#         }
        
#         if torch.cuda.is_available():
#             memory_info["gpu_memory"] = {
#                 "allocated_mb": round(torch.cuda.memory_allocated() / (1024**2), 2),
#                 "reserved_mb": round(torch.cuda.memory_reserved() / (1024**2), 2),
#                 "total_mb": round(torch.cuda.get_device_properties(0).total_memory / (1024**2), 2)
#             }
#         else:
#             memory_info["gpu_memory"] = {"available": False}
        
#         memory_info["whisper_model_loaded"] = whisper_model_manager.is_model_loaded()
#         memory_info["current_variant"] = whisper_model_manager.current_variant.value if whisper_model_manager.current_variant else None
        
#         return memory_info
        
#     except Exception as e:
#         logger.error(f"‚ùå Failed to get memory usage: {e}")
#         raise HTTPException(status_code=500, detail=f"Failed to get memory info: {str(e)}")

# # Include import for datetime at the top
# from datetime import datetime