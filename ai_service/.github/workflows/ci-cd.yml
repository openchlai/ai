name: AI Service CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'callcenter-ai/ai_service/**'
      - '.github/workflows/ai-service-*.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'callcenter-ai/ai_service/**'
  release:
    types: [ published ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  REGISTRY: ghcr.io
  IMAGE_NAME_PREFIX: ${{ github.repository }}/ai-service
  PYTHON_VERSION: '3.11'
  WORKING_DIRECTORY: ./callcenter-ai/ai_service

defaults:
  run:
    working-directory: ./callcenter-ai/ai_service

jobs:
  # Code Quality and Security Analysis
  code-quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pylint bandit safety mypy black isort pytest-cov
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Code formatting check (Black)
        run: |
          echo "ðŸŽ¨ Checking code formatting..."
          black --check --diff app/ tests/ || (echo "âŒ Code formatting issues found. Run 'black app/ tests/' to fix." && exit 1)

      - name: Import sorting check (isort)
        run: |
          echo "ðŸ“¦ Checking import sorting..."
          isort --check-only --diff app/ tests/ || (echo "âŒ Import sorting issues found. Run 'isort app/ tests/' to fix." && exit 1)

      - name: Lint with flake8
        run: |
          echo "ðŸ” Running flake8 linting..."
          flake8 app/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 app/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics

      - name: Static analysis with pylint
        run: |
          echo "ðŸ“Š Running pylint analysis..."
          pylint app/ --fail-under=7.0 --output-format=text || echo "âš ï¸ Pylint warnings found but not failing build"

      - name: Type checking with mypy
        run: |
          echo "ðŸ”§ Running type checking..."
          mypy app/ --ignore-missing-imports || echo "âš ï¸ Type checking warnings found but not failing build"

      - name: Security analysis with bandit
        run: |
          echo "ðŸ”’ Running security analysis..."
          bandit -r app/ -f json -o bandit-report.json || true
          bandit -r app/ --severity-level medium

      - name: Check for known security vulnerabilities
        run: |
          echo "ðŸ›¡ï¸ Checking for known vulnerabilities..."
          safety check --json --output safety-report.json || true
          safety check

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  # Unit and Integration Tests
  test:
    name: Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-type: [unit, integration]
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1 libsox-fmt-all sox portaudio19-dev

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-asyncio pytest-mock
          pip install -r requirements.txt

      - name: Create test environment
        run: |
          mkdir -p logs temp models
          echo "DEBUG=true" > .env.test
          echo "REDIS_URL=redis://localhost:6379/0" >> .env.test
          echo "ENABLE_MODEL_LOADING=false" >> .env.test

      - name: Run unit tests
        if: matrix.test-type == 'unit'
        run: |
          echo "ðŸ§ª Running unit tests..."
          python -m pytest tests/ -v \
            --cov=app \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term \
            --junitxml=junit/test-results.xml \
            -k "not integration"

      - name: Run integration tests
        if: matrix.test-type == 'integration'
        run: |
          echo "ðŸ”— Running integration tests..."
          python -m pytest tests/ -v \
            --cov=app \
            --cov-report=xml \
            --junitxml=junit/integration-results.xml \
            -k "integration" \
            --timeout=300

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.test-type }}
          path: |
            junit/
            htmlcov/
            .coverage
          retention-days: 30

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.test-type == 'unit'
        with:
          file: ./callcenter-ai/ai_service/coverage.xml
          flags: ai-service
          name: ai-service-coverage

  # Docker Build and Test
  docker-build:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: [code-quality, test]
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_PREFIX }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.title=AI Service
            org.opencontainers.image.description=Multi-modal AI processing pipeline
            org.opencontainers.image.vendor=CallCenter AI

      - name: Build Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./callcenter-ai/ai_service
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64

      - name: Test Docker image
        run: |
          echo "ðŸ³ Testing Docker image..."
          docker run --rm -d --name ai-service-test \
            -e DEBUG=true \
            -e ENABLE_MODEL_LOADING=false \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_PREFIX }}:latest
          
          sleep 10
          
          # Test health endpoint
          docker exec ai-service-test curl -f http://localhost:8123/health || exit 1
          
          # Test API documentation
          docker exec ai-service-test curl -f http://localhost:8123/docs || exit 1
          
          docker stop ai-service-test
          echo "âœ… Docker image tests passed"

  # Model Validation Tests
  model-tests:
    name: Model Validation
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request'
    needs: [docker-build]
    strategy:
      matrix:
        model: [whisper, translation, ner, classification, summarization]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run model validation tests
        run: |
          echo "ðŸ¤– Validating ${{ matrix.model }} model..."
          python test_runner.py --model ${{ matrix.model }} --quick-test
          
      - name: Upload model test results
        uses: actions/upload-artifact@v4
        with:
          name: model-test-${{ matrix.model }}
          path: logs/model-validation-*.json
          retention-days: 7

  # Security Scanning
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    needs: [docker-build]
    if: github.event_name != 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ needs.docker-build.outputs.image-tag }}
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run Snyk to check Docker image for vulnerabilities
        uses: snyk/actions/docker@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          image: ${{ needs.docker-build.outputs.image-tag }}
          args: --severity-threshold=high

  # Performance Testing
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: [docker-build]
    if: github.ref == 'refs/heads/main'
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install performance testing tools
        run: |
          pip install locust pytest-benchmark
          sudo apt-get update
          sudo apt-get install -y ffmpeg

      - name: Start AI Service
        run: |
          docker run -d --name ai-service-perf \
            -p 8123:8123 \
            -e DEBUG=false \
            -e ENABLE_MODEL_LOADING=false \
            -e REDIS_URL=redis://host.docker.internal:6379/0 \
            --add-host=host.docker.internal:host-gateway \
            ${{ needs.docker-build.outputs.image-tag }}
          
          sleep 30

      - name: Run performance tests
        run: |
          echo "âš¡ Running performance tests..."
          python test_runner.py --performance-test --duration 300
          
      - name: Generate performance report
        run: |
          echo "ðŸ“Š Generating performance report..."
          python -c "
          import json, datetime
          report = {
              'timestamp': datetime.datetime.now().isoformat(),
              'test_duration': '5 minutes',
              'status': 'completed'
          }
          with open('performance-report.json', 'w') as f:
              json.dump(report, f, indent=2)
          "

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results
          path: |
            performance-report.json
            logs/performance-*.log
          retention-days: 30

      - name: Cleanup
        if: always()
        run: docker stop ai-service-perf && docker rm ai-service-perf

  # Staging Deployment
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [security-scan, model-tests, performance-test]
    if: |
      (github.ref == 'refs/heads/develop' || 
       github.event.inputs.environment == 'staging') &&
      github.event_name != 'pull_request'
    environment:
      name: staging
      url: https://ai-service-staging.callcenter.ai
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to staging
        run: |
          echo "ðŸš€ Deploying AI Service to staging..."
          echo "Image: ${{ needs.docker-build.outputs.image-tag }}"
          
          # Here you would typically:
          # 1. Update Kubernetes manifests
          # 2. Apply configurations
          # 3. Verify deployment
          
          # Example deployment commands:
          # kubectl apply -f k8s/staging/
          # kubectl set image deployment/ai-service ai-service=${{ needs.docker-build.outputs.image-tag }}
          # kubectl rollout status deployment/ai-service
          
          echo "âœ… Staging deployment completed"

      - name: Run staging smoke tests
        run: |
          echo "ðŸŒŸ Running staging smoke tests..."
          # curl -f https://ai-service-staging.callcenter.ai/health
          # python scripts/smoke-test.py --environment staging
          echo "âœ… Staging smoke tests passed"

      - name: Notify deployment
        run: |
          echo "ðŸ“¢ Staging deployment notification sent"
          # Send notification to Slack, Teams, etc.

  # Production Deployment
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [security-scan, model-tests, performance-test]
    if: |
      (github.ref == 'refs/heads/main' || 
       github.event.inputs.environment == 'production') &&
      github.event_name != 'pull_request'
    environment:
      name: production
      url: https://ai-service.callcenter.ai
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to production
        run: |
          echo "ðŸš€ Deploying AI Service to production..."
          echo "Image: ${{ needs.docker-build.outputs.image-tag }}"
          
          # Production deployment commands:
          # kubectl apply -f k8s/production/
          # kubectl set image deployment/ai-service ai-service=${{ needs.docker-build.outputs.image-tag }}
          # kubectl rollout status deployment/ai-service --timeout=600s
          
          echo "âœ… Production deployment completed"

      - name: Run production health checks
        run: |
          echo "ðŸ¥ Running production health checks..."
          # curl -f https://ai-service.callcenter.ai/health/detailed
          # python scripts/health-check.py --environment production
          echo "âœ… Production health checks passed"

      - name: Update monitoring dashboards
        run: |
          echo "ðŸ“Š Updating monitoring dashboards..."
          # Update Grafana dashboards, set up alerts, etc.
          echo "âœ… Monitoring updated"

      - name: Notify successful deployment
        run: |
          echo "ðŸŽ‰ Production deployment successful!"
          # Send success notification

  # Cleanup and Reporting
  cleanup:
    name: Cleanup & Reporting
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production]
    if: always()
    steps:
      - name: Generate deployment report
        run: |
          echo "ðŸ“‹ Generating deployment report..."
          cat > deployment-report.md << 'EOF'
          # AI Service Deployment Report
          
          **Build Date:** $(date)
          **Git SHA:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          
          ## Deployment Status
          - **Staging:** ${{ needs.deploy-staging.result }}
          - **Production:** ${{ needs.deploy-production.result }}
          
          ## Quality Gates
          - **Code Quality:** ${{ needs.code-quality.result }}
          - **Tests:** ${{ needs.test.result }}
          - **Security:** ${{ needs.security-scan.result }}
          - **Performance:** ${{ needs.performance-test.result }}
          
          ## Next Steps
          - Monitor application performance
          - Review logs for any issues
          - Update documentation if needed
          EOF

      - name: Upload deployment report
        uses: actions/upload-artifact@v4
        with:
          name: deployment-report
          path: deployment-report.md
          retention-days: 90

      - name: Cleanup old artifacts
        run: |
          echo "ðŸ§¹ Cleaning up old artifacts..."
          # Logic to clean up old container images, artifacts, etc.
          echo "âœ… Cleanup completed"
